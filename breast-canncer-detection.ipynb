{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install tensorflow==2.8\n#!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n!pip install keras-cv-attention-models\n!pip install tf-models-official\n!pip install pylibjpeg\n!pip install python-gdcm\n!pip install ipywidgets\n!pip install pydicom\n!pip install -qU scikit-learn\n# !pip install --upgrade google-cloud-storage\n#!pip install --extra-index-url https://developer.download.nvidia.com/compute/redist/cuda/10.0 nvidia-dali\n#!pip install tensorflow-io\n# !pip install -U jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html","metadata":{"_uuid":"2d8440a1-7cfd-494e-a42b-3e55b49e1876","_cell_guid":"5b68ae05-8ee5-4adc-a1a2-cc55ffc2d22c","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-20T15:33:49.432916Z","iopub.execute_input":"2023-06-20T15:33:49.433185Z","iopub.status.idle":"2023-06-20T15:35:42.751967Z","shell.execute_reply.started":"2023-06-20T15:33:49.433160Z","shell.execute_reply":"2023-06-20T15:35:42.750631Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting keras-cv-attention-models\n  Downloading keras_cv_attention_models-1.3.17-py3-none-any.whl (688 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m688.6/688.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from keras-cv-attention-models) (9.5.0)\nRequirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.10/site-packages (from keras-cv-attention-models) (0.20.0)\nRequirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.10/site-packages (from keras-cv-attention-models) (4.9.2)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (from keras-cv-attention-models) (2.12.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (23.3.3)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (1.51.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (3.8.0)\nRequirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (0.4.8)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (2.12.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (16.0.0)\nRequirement already satisfied: numpy<1.24,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (1.23.5)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (59.8.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (1.16.0)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (2.12.3)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (2.12.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (2.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (4.5.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->keras-cv-attention-models) (0.31.0)\nRequirement already satisfied: typeguard<3.0.0,>=2.7 in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons->keras-cv-attention-models) (2.13.3)\nRequirement already satisfied: array-record in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv-attention-models) (0.2.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv-attention-models) (8.1.3)\nRequirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv-attention-models) (0.1.8)\nRequirement already satisfied: etils[enp,epath]>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv-attention-models) (1.2.0)\nRequirement already satisfied: promise in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv-attention-models) (2.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv-attention-models) (5.9.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv-attention-models) (2.28.2)\nRequirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv-attention-models) (0.14.0)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv-attention-models) (0.10.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv-attention-models) (4.64.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow->keras-cv-attention-models) (0.40.0)\nRequirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->keras-cv-attention-models) (5.12.0)\nRequirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->keras-cv-attention-models) (3.15.0)\nRequirement already satisfied: ml-dtypes>=0.0.3 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow->keras-cv-attention-models) (0.1.0)\nRequirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow->keras-cv-attention-models) (1.10.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv-attention-models) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv-attention-models) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv-attention-models) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv-attention-models) (2023.5.7)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (2.17.3)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (3.4.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (0.7.0)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (2.3.6)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow->keras-cv-attention-models) (3.0.9)\nRequirement already satisfied: googleapis-common-protos in /opt/conda/lib/python3.10/site-packages (from tensorflow-metadata->tensorflow-datasets->keras-cv-attention-models) (1.57.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (2.1.2)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (3.2.2)\nInstalling collected packages: keras-cv-attention-models\nSuccessfully installed keras-cv-attention-models-1.3.17\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting tf-models-official\n  Downloading tf_models_official-2.12.0-py2.py3-none-any.whl (2.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: Cython in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (0.29.34)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (9.5.0)\nCollecting gin-config (from tf-models-official)\n  Downloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: google-api-python-client>=1.6.7 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (2.88.0)\nCollecting immutabledict (from tf-models-official)\n  Downloading immutabledict-2.2.4-py3-none-any.whl (4.1 kB)\nRequirement already satisfied: kaggle>=1.3.9 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (1.5.13)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (3.6.3)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (1.23.5)\nRequirement already satisfied: oauth2client in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (4.1.3)\nRequirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (4.7.0.72)\nRequirement already satisfied: pandas>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (1.5.3)\nRequirement already satisfied: psutil>=5.4.3 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (5.9.3)\nRequirement already satisfied: py-cpuinfo>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (9.0.0)\nCollecting pycocotools (from tf-models-official)\n  Downloading pycocotools-2.0.6.tar.gz (24 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pyyaml<6.0,>=5.1 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (5.4.1)\nCollecting sacrebleu (from tf-models-official)\n  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (1.10.1)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (0.1.99)\nCollecting seqeval (from tf-models-official)\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (1.16.0)\nRequirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (0.20.0)\nRequirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (4.9.2)\nRequirement already satisfied: tensorflow-hub>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (0.12.0)\nCollecting tensorflow-model-optimization>=0.4.1 (from tf-models-official)\n  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorflow-text~=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (2.12.1)\nRequirement already satisfied: tensorflow~=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (2.12.0)\nCollecting tf-slim>=1.1.0 (from tf-models-official)\n  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.21.0)\nRequirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.17.3)\nRequirement already satisfied: google-auth-httplib2>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.1.0)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (1.33.2)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (3.0.1)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (2023.5.7)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (2.8.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (2.28.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (4.64.1)\nRequirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (8.0.1)\nRequirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (1.26.15)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.22.0->tf-models-official) (2023.3)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.12.0->tf-models-official) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.12.0->tf-models-official) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.12.0->tf-models-official) (23.3.3)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.12.0->tf-models-official) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.12.0->tf-models-official) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.12.0->tf-models-official) (1.51.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.12.0->tf-models-official) (3.8.0)\nRequirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.12.0->tf-models-official) (0.4.8)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.12.0->tf-models-official) (2.12.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.12.0->tf-models-official) (16.0.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.12.0->tf-models-official) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.12.0->tf-models-official) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.12.0->tf-models-official) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.12.0->tf-models-official) (59.8.0)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.12.0->tf-models-official) (2.12.3)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.12.0->tf-models-official) (2.12.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.12.0->tf-models-official) (2.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.12.0->tf-models-official) (4.5.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.12.0->tf-models-official) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.12.0->tf-models-official) (0.31.0)\nRequirement already satisfied: dm-tree~=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.8)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official) (1.0.7)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official) (4.39.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official) (1.4.4)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official) (3.0.9)\nRequirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official) (0.4.8)\nRequirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official) (0.2.7)\nRequirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official) (4.9)\nCollecting portalocker (from sacrebleu->tf-models-official)\n  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official) (2023.5.5)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official) (4.9.2)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval->tf-models-official) (1.2.2)\nRequirement already satisfied: typeguard<3.0.0,>=2.7 in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons->tf-models-official) (2.13.3)\nRequirement already satisfied: array-record in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (0.2.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (8.1.3)\nRequirement already satisfied: etils[enp,epath]>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (1.2.0)\nRequirement already satisfied: promise in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (2.3)\nRequirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (0.14.0)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (0.10.2)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow~=2.12.0->tf-models-official) (0.40.0)\nRequirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tf-models-official) (5.12.0)\nRequirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tf-models-official) (3.15.0)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official) (1.57.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official) (4.2.4)\nRequirement already satisfied: ml-dtypes>=0.0.3 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow~=2.12.0->tf-models-official) (0.1.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle>=1.3.9->tf-models-official) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle>=1.3.9->tf-models-official) (3.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (3.1.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-models-official) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-models-official) (3.4.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-models-official) (0.7.0)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-models-official) (2.3.6)\nRequirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-models-official) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-models-official) (2.1.2)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.12.0->tf-models-official) (3.2.2)\nBuilding wheels for collected packages: pycocotools, seqeval\n  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp310-cp310-linux_x86_64.whl size=93512 sha256=00f226590afaec320efb38b8c59dcc84711ecd645bbf1b6d1469e801b2681304\n  Stored in directory: /root/.cache/pip/wheels/58/e6/f9/f87c8f8be098b51b616871315318329cae12cdb618f4caac93\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=468f9abc0a3c0390ddfacf21e82c3c10523e73bd94deeb2858b568ba990feb45\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built pycocotools seqeval\nInstalling collected packages: gin-config, tf-slim, tensorflow-model-optimization, portalocker, immutabledict, sacrebleu, seqeval, pycocotools, tf-models-official\nSuccessfully installed gin-config-0.5.0 immutabledict-2.2.4 portalocker-2.7.0 pycocotools-2.0.6 sacrebleu-2.3.1 seqeval-1.2.2 tensorflow-model-optimization-0.7.5 tf-models-official-2.12.0 tf-slim-1.1.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting pylibjpeg\n  Downloading pylibjpeg-1.4.0-py3-none-any.whl (28 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pylibjpeg) (1.23.5)\nInstalling collected packages: pylibjpeg\nSuccessfully installed pylibjpeg-1.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting python-gdcm\n  Downloading python_gdcm-3.0.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: python-gdcm\nSuccessfully installed python-gdcm-3.0.22\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (7.7.1)\nRequirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (6.23.0)\nRequirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (0.2.0)\nRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (5.9.0)\nRequirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (3.6.4)\nRequirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (8.13.2)\nRequirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (3.0.7)\nRequirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\nRequirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.7)\nRequirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.4.9)\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.0)\nRequirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.3)\nRequirement already satisfied: pyzmq>=20 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (25.0.2)\nRequirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.1)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.2)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.38)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (2.15.1)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (0.6.2)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\nRequirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.10/site-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.4)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\nRequirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\nRequirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.5.0)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.2)\nRequirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.3.0)\nRequirement already satisfied: nbformat in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.8.0)\nRequirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.4.5)\nRequirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.2)\nRequirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.17.1)\nRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.16.0)\nRequirement already satisfied: nbclassic>=0.4.7 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.0.0)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=4.0.0->ipywidgets) (0.2.6)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->ipykernel>=4.5.1->ipywidgets) (3.0.9)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (1.2.0)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (2.2.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.2.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=4.0.0->ipywidgets) (1.16.0)\nRequirement already satisfied: jupyter-server>=1.8 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.5.0)\nRequirement already satisfied: notebook-shim>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.3)\nRequirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\nRequirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.2)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.0.0)\nRequirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)\nRequirement already satisfied: testpath in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.6.0)\nRequirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.12.2)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.13)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.1.2)\nRequirement already satisfied: fastjsonschema in /opt/conda/lib/python3.10/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.16.3)\nRequirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.10/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.17.3)\nRequirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\nRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.19.3)\nRequirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.6.2)\nRequirement already satisfied: jupyter-events>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.6.3)\nRequirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.4.4)\nRequirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\nRequirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.15.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.3.2.post1)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.4)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21)\nRequirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.4.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.0.7)\nRequirement already satisfied: pyyaml>=5.3 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.4.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.4.1)\nRequirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.4.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.4)\nRequirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.4.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.1)\nRequirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\nRequirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (20.11.0)\nRequirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.0)\nRequirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.0)\nRequirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.13)\nRequirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: pydicom in /opt/conda/lib/python3.10/site-packages (2.3.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# import tensorflow as tf\n# tf.keras.backend.clear_session()\n# try:\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver(\"local\")  # TPU detection\n#     print('Running on TPU ', tpu.master())\n# except ValueError:\n#     raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n# strategy = tf.distribute.TPUStrategy(tpu)","metadata":{"_uuid":"b48d2e02-89f7-4c27-a02a-04e2b9469b38","_cell_guid":"a8f579a1-baa2-4fb1-896e-b1ce14a721e6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-20T15:35:42.754162Z","iopub.execute_input":"2023-06-20T15:35:42.754842Z","iopub.status.idle":"2023-06-20T15:35:42.761911Z","shell.execute_reply.started":"2023-06-20T15:35:42.754804Z","shell.execute_reply":"2023-06-20T15:35:42.760699Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"_uuid":"8d7d12b2-cc80-4096-b2df-70920ca8a4bb","_cell_guid":"40c0f035-cf0a-492d-8e44-7e3a32f67e24","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-20T15:35:42.764244Z","iopub.execute_input":"2023-06-20T15:35:42.764770Z","iopub.status.idle":"2023-06-20T15:35:43.851922Z","shell.execute_reply.started":"2023-06-20T15:35:42.764738Z","shell.execute_reply":"2023-06-20T15:35:43.850728Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Tue Jun 20 15:35:43 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"\nimport tensorflow as tf\nprint(tf.test.is_gpu_available())\nprint(tf.test.is_built_with_cuda())\nimport os\n#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n#os.environ['TF_XLA_FLAGS'] = '--tf_xla_cpu_global_jit'\n#from keras_cv_attention_models import convnext, efficientnet, swin_transformer_v2\nimport pandas as pd\nfrom pydicom import dcmread\n#import Pillow\nimport pylibjpeg\nimport numpy as np\nimport cv2 \nimport gc \nimport gdcm\nimport matplotlib.pyplot as plt\nimport matplotlib.image\nfrom sklearn.model_selection import train_test_split\n#from tensorflow.keras import initializers\n#import official.nlp.modeling.layers as nlp_layers","metadata":{"_uuid":"591cdd1a-cd19-40db-8e77-eda84b009913","_cell_guid":"a258cfd7-0703-44eb-9868-fca8f4ab3919","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-20T15:35:43.856655Z","iopub.execute_input":"2023-06-20T15:35:43.856969Z","iopub.status.idle":"2023-06-20T15:35:55.341190Z","shell.execute_reply.started":"2023-06-20T15:35:43.856942Z","shell.execute_reply":"2023-06-20T15:35:55.340203Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"True\nTrue\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nimport time\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  # Restrict TensorFlow to only use the first GPU\n    try:\n        tf.config.set_visible_devices(gpus, 'GPU')\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.list_logical_devices('GPU')\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n        strategy = tf.distribute.MirroredStrategy()\n        num_replicas_in_sync = strategy.num_replicas_in_sync\n        print('number of replicas: ',num_replicas_in_sync)\n    except RuntimeError as e:\n        print(e)\nelse:\n    num_replicas_in_sync = 1\n    strategy = tf.distribute.get_strategy()","metadata":{"_uuid":"0805b029-e097-4889-b0e1-8e6b8e3ade79","_cell_guid":"a0869ede-3c38-4982-9273-8121c37ff8e1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-20T15:35:55.342787Z","iopub.execute_input":"2023-06-20T15:35:55.343661Z","iopub.status.idle":"2023-06-20T15:35:55.774353Z","shell.execute_reply.started":"2023-06-20T15:35:55.343619Z","shell.execute_reply":"2023-06-20T15:35:55.773261Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"1 Physical GPUs, 1 Logical GPU\nnumber of replicas:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"class Distribute_train_vali_data():\n    def __init__(self, total_data_points, train_vali_ratio, random_object):\n        self.total_data_points = total_data_points\n        self.train_vali_ratio = train_vali_ratio\n        self.random_object = random_object\n        \n    def _get_pair(self,input_tensor, element_position, duplicate_constant, reduce_shape):\n        \"\"\"element_position = index in list\"\"\"\n        _new_length = reduce_shape - (element_position+1)\n        \n        left_list = input_tensor[element_position+1:reduce_shape,0:2]    #left_list: the remain tensors in list\n        density_list = input_tensor[element_position+1:reduce_shape,4:5]\n        \n        current_value = input_tensor[element_position]\n        remained_elements = tf.concat([left_list, density_list],axis = 1)\n        \n        #Create duplcate information\n        \"\"\"\n        Example:\n                        \"dup_infor\" \"index_data\"\n        duplicated_index =  [[3     ,    0]   --> row index 0 repeats 3 times\n                             [2     ,    1]   --> row index 1 repeats 2 times\n                             [5     ,    2]   --> row index 2 repeats 5 times\n                             [4     ,    3]]  --> row index 3 repeats 4 times\n        \n        \"\"\"\n        dup_infor = tf.multiply(tf.add(current_value[4],density_list),duplicate_constant)   #from density level of current element and other elements\n        index_data = tf.reshape(tf.range(_new_length), shape = (_new_length,1) )\n        duplicated_index = tf.map_fn(lambda x: tf.fill(dims = [x[0]], value = x[1]), \n                                 elems = tf.concat([dup_infor, index_data],axis = 1),\n                                 fn_output_signature = tf.RaggedTensorSpec(shape= [None], dtype=tf.int32) )\n        \"\"\"\n        Example:\n        duplicated_index = [0,0,0,1,1,2,2,2,2,2,4,4,4]\n        \"\"\"\n        duplicated_index = duplicated_index.merge_dims(0,1)\n        \n        return tf.map_fn(lambda x: tf.concat([current_value,x],axis = 0) , elems = tf.gather(params = remained_elements, indices = duplicated_index, axis = 0))\n\n    def _query_lateral_tensors_from_frame(self,data_frame):\n        \"\"\"Query element in dataframe and convert into tensor\"\"\"\n        Left_list = data_frame.query( 'laterality == 0', inplace = False)\n        Left_tensors = tf.convert_to_tensor(Left_list.values, dtype = tf.int32)\n        Right_list = data_frame.query( 'laterality == 1', inplace = False)\n        Right_tensors = tf.convert_to_tensor(Right_list.values, dtype = tf.int32)\n        return Left_tensors, Right_tensors\n    \n    def _get_mix_tensors_for_training(self,pos_train_frame, neg_train_frame, duplicate_constant):\n        \"\"\"Inputs: positive class and negative class in dataframe\"\"\"\n        Left_tensors , Right_tensors = self._query_lateral_tensors_from_frame(pos_train_frame)\n        \n        ini_length_L = Left_tensors.shape[0]\n        ini_length_R = Right_tensors.shape[0]\n        tf.print(\"Befor applying MixUP method, in positive class: \")\n        tf.print(\"   Laterality L view: \",ini_length_L,\"images\")\n        tf.print(\"   Laterality R view: \",ini_length_R,\"images\")\n        Negative_class_tensor = tf.convert_to_tensor(neg_train_frame.values, dtype = tf.int32) \n        \n        #Create pairs of elements \n        Left_Positive = tf.concat([self._get_pair(Left_tensors, element_position = i, duplicate_constant = duplicate_constant, reduce_shape = ini_length_L) for i in range(ini_length_L-3)], axis = 0)\n        Right_Positive = tf.concat([self._get_pair(Right_tensors, element_position = i, duplicate_constant = duplicate_constant, reduce_shape = ini_length_R) for i in range(ini_length_R-3)], axis = 0)\n        Negative_class_tensor = tf.concat([Negative_class_tensor, tf.zeros(shape = (Negative_class_tensor.shape[0],3), dtype = tf.int32) ], axis = 1)\n        \n        #Add zeros (no argument) to initail tensors\n        Left_tensors = tf.concat([Left_tensors, tf.zeros(shape = (Left_tensors.shape[0],3), dtype = tf.int32)], axis = 1)\n        Right_tensors = tf.concat([Right_tensors, tf.zeros(shape = (Right_tensors.shape[0],3), dtype = tf.int32)], axis = 1)\n        #Concat with initial\n        Left_final = tf.concat([Left_Positive,Left_tensors],axis = 0)\n        Right_final = tf.concat([Right_Positive,Right_tensors],axis = 0)\n        #Target length for Left and Right tensor \n        target = int(Negative_class_tensor.shape[0]//2)\n        #Select elements to meet target lenght \n        target_length_L = tf.subtract(target, ini_length_L)\n        if target_length_L < Left_final.shape[0]:\n            current_indices_L = tf.random.shuffle(tf.range(Left_final.shape[0]),seed = 100)\n            #tf.print(\"size of indices L: \",current_indices_L.shape)\n            reduced_indices_L = tf.slice(current_indices_L,[1],[target_length_L])\n            Left_final = tf.gather(params = Left_final,indices = reduced_indices_L,axis = 0)\n        \n        target_length_R = tf.subtract(target, ini_length_R)\n        if target_length_R < Right_final.shape[0]:\n            current_indices_R = tf.random.shuffle(tf.range(Right_final.shape[0]),seed = 80)\n            #tf.print(\"size of indices R: \",target_length_R.shape)\n            reduced_indices_R = tf.slice(current_indices_R,[1],[target_length_R])\n            Right_final = tf.gather(params = Right_final,indices = reduced_indices_R,axis = 0)\n        \n        tf.print(\"------------------------------------------------\")\n        tf.print(\"After applying MixUP method, in positive class: \")\n        tf.print(\"   Laterality L view: \",Left_final.shape[0],\"images\")\n        tf.print(\"   Laterality R view: \",Right_final.shape[0],\"images\")\n        tf.print(\"After applying MixUP method: positive class: \",Left_final.shape[0]+Right_final.shape[0],\"images, negative class: \",Negative_class_tensor.shape[0],\"images\")\n        return tf.concat([Left_final, Right_final, Negative_class_tensor], axis = 0)\n\n    def _get_tensors_for_validation(self,pos_vali_frame,neg_vali_frame):\n        pos_class_tensor = tf.convert_to_tensor(pos_vali_frame.values, dtype = tf.int32) \n        neg_class_tensor = tf.convert_to_tensor(neg_vali_frame.values, dtype = tf.int32) \n        pos_class_tensor = tf.concat([pos_class_tensor, tf.zeros(shape = (pos_class_tensor.shape[0],3), dtype = tf.int32) ], axis = 1)\n        neg_class_tensor = tf.concat([neg_class_tensor, tf.zeros(shape = (neg_class_tensor.shape[0],3), dtype = tf.int32) ], axis = 1)\n        return tf.concat([pos_class_tensor, neg_class_tensor], axis = 0)\n    \n    def _split_pos_class(self,Total_dataframe):\n        #Select True Class\n        true_class_L = Total_dataframe.query( 'cancer == 1 and laterality == 0', inplace = False)\n        true_class_R = Total_dataframe.query( 'cancer == 1 and laterality == 1', inplace = False)\n        #Split directory into train dir and vali dir\n        train_true_class_L, vali_true_class_L = train_test_split(true_class_L,\n                                               test_size = self.train_vali_ratio,\n                                               random_state = self.random_object)\n        #Reset index of both sets\n        train_true_class_L.reset_index(drop= True, inplace = True)\n        vali_true_class_L.reset_index(drop= True, inplace = True)\n  \n        train_true_class_R, vali_true_class_R = train_test_split(true_class_R,\n                                               test_size = self.train_vali_ratio,\n                                               random_state = self.random_object)\n        #Reset index of both sets\n        train_true_class_R.reset_index(drop= True, inplace = True)\n        vali_true_class_R.reset_index(drop= True, inplace = True)\n        \n        #Finalize\n        train_true_class = pd.concat([train_true_class_L, train_true_class_R], axis = 0, ignore_index=True)\n        vali_true_class = pd.concat([vali_true_class_L, vali_true_class_R], axis = 0, ignore_index=True)\n    \n        #Return dataframe\n        return train_true_class, vali_true_class\n    \n    def _split_neg_class(self,Total_dataframe):\n        #Select True Class\n        false_class = Total_dataframe.query( 'cancer == 0', inplace = False)\n        #Split directory into train dir and vali dir\n        train_false_class, vali_false_class = train_test_split(false_class,\n                                               test_size= self.train_vali_ratio,\n                                               random_state=self.random_object)\n        #Reset index of both sets\n        train_false_class.reset_index(drop= True, inplace = True)\n        vali_false_class.reset_index(drop= True, inplace = True)\n        #Return dataframe\n        return train_false_class, vali_false_class\n\n    #@tf.function\n    def make_img_dir_list(self):\n        #Read csv\n        train_list = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/train.csv')\n        #Make directory list\n        img_dim_list=pd.DataFrame(data={'patient_id': train_list.patient_id\n                                    ,'image_id': train_list.image_id\n                                    ,'laterality':train_list.laterality\n                                    ,'cancer': train_list.cancer\n                                    , 'density':train_list.density}\n                              ,columns = ['patient_id','image_id','laterality','cancer','density'])\n        img_dim_list.density.fillna(0, inplace=True)\n        #Convert string value to integer\n        #'laterality': 'L'->0, 'R'->1\n        #'density': 'A'->1,'B'->2,'C'->3,'D'->4  \n        img_dim_list = img_dim_list.replace(['L','R','A','B','C','D'], [0,1,1,2,3,4])\n        selected_img_list = img_dim_list.sample(self.total_data_points)\n        \n        #Split positive and negative class respectively\n        pos_train, pos_vali = self._split_pos_class(selected_img_list)\n        neg_train, neg_vali = self._split_neg_class(selected_img_list)\n        tf.print(\"Before applying MixUP method: positive class : \",pos_train.shape[0],\"images,\",\" negative class: \",neg_train.shape[0],\"images\")\n        tf.print(\"Before applying MixUP method: total size of training set : \",pos_train.shape[0]+neg_train.shape[0],\"images\")\n        #Argumentation on train data of 'True' class\n        argumented_training_tensor = self._get_mix_tensors_for_training(pos_train,neg_train , duplicate_constant = 2)  \n        tf.print(\"After applying MixUP method: total size of training set: \", argumented_training_tensor.shape[0],\"images\")\n        \n        #Add zeors into new columns\n        validation_tensor = self._get_tensors_for_validation(pos_vali,neg_vali)\n        tf.print(\"------------------------------------------------\")\n        tf.print(\"Total size of validation set: \",validation_tensor.shape[0], \"images\")\n        tf.print(\"include: positive class: \",pos_vali.shape[0],\"images,\",\" negative class: \",neg_vali.shape[0],\"images\")\n        #Return tensor\n        tf.print(\"End create training and validation tensor\")\n        return argumented_training_tensor, validation_tensor","metadata":{"_uuid":"766a89f3-e38f-4731-9f39-cb8549b6ed67","_cell_guid":"5247f04f-cdab-4f96-ba0c-691931e8b1af","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-20T15:35:55.775968Z","iopub.execute_input":"2023-06-20T15:35:55.776525Z","iopub.status.idle":"2023-06-20T15:35:55.816608Z","shell.execute_reply.started":"2023-06-20T15:35:55.776489Z","shell.execute_reply":"2023-06-20T15:35:55.815690Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class Make_TFRecord(Distribute_train_vali_data):\n    def __init__(self,Number_of_cases,target_img_height, target_img_width,train_vali_ratio, dtype, epochs, random_object):\n        \"\"\"\n        Class for reading data from original format e.g .dcm file with tensor list containts patient_id and image_id. \n        Then decode in jpeg as bytestring to TFRecord files\n        \"\"\"\n        self.random_object = random_object\n        super().__init__(Number_of_cases,train_vali_ratio,self.random_object)\n        self.main_dtype = dtype\n        self.epochs = epochs\n        self.target_img_height = target_img_height\n        self.target_img_width = target_img_width\n\n        self.zero_scalar = tf.constant(0, dtype = tf.int32, shape = ())\n        self.one_scalar = tf.constant(1, dtype = tf.int32, shape = ())\n        self._initialize_dataset()\n        self._current_TFrecord_path = \"\"\n    \n    def _split_value(self, row):\n        \"\"\"Split a row of orginanl data into patiend_id and image_id\"\"\"\n        return tf.slice(row, [0],[1]), tf.slice(row, [1],[1]) \n    \n    def _split_values_of_two_images(self,row):\n        return  tf.slice(row, [0],[1]), tf.slice(row, [1],[1]), tf.slice(row, [5],[1]), tf.slice(row, [6],[1]) \n    \n    def _dicom_to_tensor(self, patient_id, img_id):\n        \"\"\"\n        Read dicom file and return bytes array\n        Input: \n        patient_id: tensor type int\n        img_id: tensor type int\n        Output:\n        numpy array of image\n        \"\"\"\n        patient_id = tf.reshape(patient_id, shape = ())\n        img_id = tf.reshape(img_id, shape = ())\n        #tf.print(\"value: \",patient_id,\" ,type: \",patient_id.dtype)\n        train_default_path = f'/kaggle/input/rsna-breast-cancer-detection/train_images/'\n        string_tensor_path = tf.strings.format(train_default_path+'{}/{}.dcm', (patient_id,img_id))\n        \n        string_np_path = string_tensor_path.numpy().decode()\n        dcm_file = dcmread(string_np_path, force = True)\n        image_array = dcm_file.pixel_array  #Output from extract dicom is uint16\n        image_array = image_array.reshape((image_array.shape[0],image_array.shape[1],1))\n        return tf.cast(tf.image.resize(image_array,size = (self.target_img_height,self.target_img_width)), dtype = tf.uint16)\n    \n    def _get_an_image_bytes(self, pa, img_id):\n        \"\"\"Use for both cases\"\"\"\n        img_tensor = tf.py_function(func=self._dicom_to_tensor, inp=[pa, img_id], Tout=tf.uint16)\n        \n        # Normalize to 0 and 1 value\n        temp_img = tf.math.divide(img_tensor,tf.constant(32767,dtype = img_tensor.dtype))\n        \n        # Convert to unit8 range from 0 to 255\n        img_tensor = tf.math.multiply(img_tensor, tf.constant(255,dtype = img_tensor.dtype))\n        \n        # Ready to cast to uint8\n        img_tensor = tf.cast(img_tensor, dtype = tf.uint8)\n        \n        tf.debugging.assert_greater(tf.shape(tf.shape(img_tensor)),2)\n        \n        buff_tensor = tf.io.encode_jpeg(img_tensor,format='grayscale',quality=95,progressive=False,optimize_size=True, chroma_downsampling=True,density_unit='in',x_density=300, y_density=300,xmp_metadata='')\n        return tf.reshape(buff_tensor, shape = (1,))  #Shape = (1,) for creating data.Dataset\n    \n    def _condition_function_argument(self, a_row):\n        return tf.math.equal(tf.slice(a_row,[5],[1]), self.zero_scalar)\n    \n    def _true_function_argument(self, a_row):\n        \"\"\"\n        Use for normal case, no need mixup\n        \"\"\"\n        pa,img_id = self._split_value(a_row)\n        #Return format: image btyes of main image, image btyes of main image, key =1 for SKIPPING second img bytes\n        return self._get_an_image_bytes(pa,img_id), tf.constant([b\"\\x00\"], dtype=tf.string), tf.constant([1],dtype = tf.int32)\n    \n    def _false_function_argument(self, a_row):\n        \"\"\"\n        Use for argumentation case\n        \"\"\"\n        pa1,img_id1, pa2, img_id2 = self._split_values_of_two_images(a_row)\n        img_bytes_1 = self._get_an_image_bytes(pa1,img_id1)\n        img_bytes_2 = self._get_an_image_bytes(pa2,img_id2)\n        \n        #Return format: image btyes of main image, image btyes of paired image, key =0 for READING second img bytes\n        return img_bytes_1 , img_bytes_2, tf.constant([0],dtype = tf.int32)\n    \n    @tf.function\n    def _create_a_tuple(self,an_element):    #map function for Dataset\n        tf.debugging.assert_equal(an_element.shape, 8, message=\"element's shape is not equal 8\")\n        #tf.print(\"Input row: \", an_element,\", with type: \",an_element.dtype)\n        #Condition to check need to do argument or not\n        image_bytes_1, image_bytes_2, key= tf.cond(self._condition_function_argument(an_element),\n                           lambda: self._true_function_argument(an_element),\n                           lambda: self._false_function_argument(an_element))\n        \n        #Make label for each image\n        def assign_label_1():\n            #Class 0 for having no cancer\n            return tf.constant([0], dtype=tf.int32)\n        def assign_label_2():\n            #Class 1 for having cancer\n            return tf.constant([1], dtype=tf.int32)\n        \n        #Compare and assign label\n        temp_label = tf.cond( tf.equal(tf.slice(an_element,[3],[1]), tf.constant(0)), assign_label_1, assign_label_2 )\n        #tf.print(\"Returned a dataset\")\n        return tf.data.Dataset.from_tensor_slices((image_bytes_1, temp_label, image_bytes_2, key))\n    \n    def _create_dataset(self, input_dataset, batch_size):\n        return input_dataset.interleave(lambda single_row: self._create_a_tuple(single_row),\n                                                       cycle_length = batch_size,\n                                                       block_length = batch_size,\n                                                       num_parallel_calls=tf.data.AUTOTUNE)\n    \n    # Three types of data can be stored in TFRecords: bytestrings, integers and floats\n    # They are always stored as lists, a single data element will be a list of size 1\n    def _bytestring_feature(self,list_of_bytestrings):\n        return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))\n    \n    def _int_feature(self,list_of_ints): # int64\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\n\n    def _float_feature(self,list_of_floats): # float32\n        return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))\n\n    def _make_example(self, main_img_bytes, label, paired_img_bytes, key):\n        feature = {\n            \"image\": self._bytestring_feature([main_img_bytes.numpy()]),          # one image in the list\n            \"class\": self._int_feature([label]),                                  # one class in the list\n            \"paired_image\": self._bytestring_feature([paired_img_bytes.numpy()]), # one image in the list\n            \"key\": self._int_feature([key])                                       # one key in the list\n        }\n        return tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n    \n    @tf.function\n    def tf_example(self,main_img_bytes, label, paired_img_bytes, key):\n        return tf.py_function(self._make_example,[main_img_bytes, label, paired_img_bytes, key],tf.string)\n    \n    def _initialize_dataset(self):\n        train_dir_tensors, vali_dir_tensors = self.make_img_dir_list()\n        self.train_dataset = tf.data.Dataset.from_tensor_slices(train_dir_tensors)\n        self.vali_dataset = tf.data.Dataset.from_tensor_slices(vali_dir_tensors)\n        self.train_length = train_dir_tensors.shape[0]\n        self.vali_length = vali_dir_tensors.shape[0]\n        del train_dir_tensors\n        del vali_dir_tensors\n        gc.collect()        \n        \n    # Wrapped by tf.py_function, execute in eager mode only\n    def _write_a_record(self, contents,batch_order,batch_size_writing_file):\n        temp_dataset = tf.data.Dataset.from_tensor_slices(contents)\n        tf.print(\"Size of content: \",tf.shape(contents))\n        temp_dataset = temp_dataset.batch(1) #read single row\n        #tfrec_file_dir: string tensor\n        tfrec_file_dir = tf.strings.join([self._current_TFrecord_path,tf.as_string(batch_order),\"_\",tf.as_string(batch_size_writing_file),\".tfrec\"])\n        \n        Writer_options = tf.io.TFRecordOptions(compression_type='GZIP', compression_level=2)\n        with tf.io.TFRecordWriter(tfrec_file_dir.numpy().decode(),options = Writer_options) as file_writer:\n            #loop through each element in batch for each file\n            for single_row in temp_dataset:\n                file_writer.write(single_row.numpy()[0])\n            file_writer.flush()\n        return None\n    \n    @tf.function(autograph = True)\n    def _write_a_record_run_func(self,batch_elements,batch_order,current_batch_size):\n        tf.py_function(func = self._write_a_record,inp = [batch_elements,batch_order,current_batch_size], Tout = [])\n    \n    def _update_TFrecord_path(self, new_TFrecord_path):\n        \"\"\"new_TFrecord_path: python string\"\"\"\n        self._current_TFrecord_path = new_TFrecord_path\n    \n    #Normal python function\n    def write_to_tfrecord(self,input_dataset, current_batch_size, string_tensor_data_dir, limit_batch_order, strategy):         \n        interleaved_dataset = self._create_dataset(input_dataset, current_batch_size)\n        #Update save directory of TFrecord file\n        self._update_TFrecord_path(string_tensor_data_dir)\n        print(\"Start Writing TFRecords\")\n        #Map elements in interleaved_dataset to features of the example\n        serialized_features_dataset = interleaved_dataset.map(self.tf_example, num_parallel_calls=tf.data.AUTOTUNE)\n        \n        serialized_features_dataset = serialized_features_dataset.batch(current_batch_size,drop_remainder=True)\n        serialized_features_dataset = serialized_features_dataset.prefetch(buffer_size = 3)\n        #Distributed the dataset over devices\n        dist_dataset = strategy.experimental_distribute_dataset(serialized_features_dataset)\n        \n        for batch_order, batch_elements in enumerate(dist_dataset):\n            if batch_order < limit_batch_order:\n                tf.print(\"Current batch order: \",batch_order)\n                strategy.run(self._write_a_record_run_func,args=(batch_elements,\n                                                                 tf.constant(batch_order, dtype = tf.int32),\n                                                                 tf.constant(current_batch_size, dtype = tf.int32)))\n            else:\n                break\n            batch_elements = None\n            del batch_elements\n        del serialized_features_dataset, interleaved_dataset\n        gc.collect()","metadata":{"_uuid":"706815be-66d2-4fe7-b5eb-30a44165aacd","_cell_guid":"6bc32740-846a-42c6-8279-004e29f58a97","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-20T15:35:55.818111Z","iopub.execute_input":"2023-06-20T15:35:55.818452Z","iopub.status.idle":"2023-06-20T15:35:55.861262Z","shell.execute_reply.started":"2023-06-20T15:35:55.818419Z","shell.execute_reply":"2023-06-20T15:35:55.860383Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# # patient_id = 22895\n# # img_id = 1002924244\n\n# patient_id = 10011\n# img_id = 1031443799\n\n# train_default_path = f'/kaggle/input/rsna-breast-cancer-detection/train_images/'\n# string_tensor_path = tf.strings.format(train_default_path+'{}/{}.dcm', (patient_id,img_id))\n        \n# string_np_path = string_tensor_path.numpy().decode()\n# dcm_file = dcmread(string_np_path, force = True)\n\n# offset = dcm_file.PixelData.find(b\"\\x00\\x00\\x00\\x0C\")\n# # ini_pixel = dcm_file.pixel_array\n# test_pixel = dcm_file.pixel_array\n# tf.print(test_pixel.shape)\n# main_img = tf.constant(test_pixel,shape = (test_pixel.shape[0], test_pixel.shape[1],1),dtype = tf.uint8)\n\n# buff_tensor = tf.io.encode_jpeg(main_img,format='grayscale',quality=80,\n#                                 progressive=True,optimize_size=True, \n#                                 chroma_downsampling=True,density_unit='in',x_density=300, y_density=300,xmp_metadata='')\n\n# tf.print(\"String type: \",tf.io.is_jpeg(buff_tensor))\n\n# decoded_image = tf.io.decode_jpeg(buff_tensor,channels=1,ratio=1,\n#                                          fancy_upscaling=True,\n#                                          try_recover_truncated=True,\n#                                          acceptable_fraction=1,dct_method='')\n# decoded_image = tf.cast(decoded_image, dtype = tf.uint8)\n\n# tf.print(decoded_image.numpy().shape)\n# tf.print(np.array_equal(main_img, decoded_image.numpy()))\n# plt.imshow(decoded_image.numpy(),cmap =\"gray\")\n# plt.show()\n\n\n# feature = {\"image\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[buff_tensor.numpy()] )) }\n# an_example = tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n\n# data = tf.data.Dataset.from_tensor_slices([an_example, an_example])\n\n# data = data.batch(1) #read single row\n        \n# tfrec_file_dir = tf.strings.join([\"/kaggle/working/1_dump.tfrec\"])\n        \n# # Writer_options = tf.io.TFRecordOptions(compression_type='GZIP', compression_level=9) #options = Writer_options\n# # with tf.io.TFRecordWriter(tfrec_file_dir.numpy().decode()) as file_writer:\n# #     #loop through each element in batch for each file\n# #     for single_row in data:\n# #         file_writer.write(single_row.numpy()[0])\n# #         file_writer.flush()","metadata":{"_uuid":"4bce3958-9382-4056-94bd-43b09a3cde1c","_cell_guid":"557e82dd-59cf-4a68-a7d2-4d3ba61d1a94","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-20T15:35:55.862700Z","iopub.execute_input":"2023-06-20T15:35:55.862991Z","iopub.status.idle":"2023-06-20T15:35:55.874136Z","shell.execute_reply.started":"2023-06-20T15:35:55.862968Z","shell.execute_reply":"2023-06-20T15:35:55.873131Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Main_Architecture(tf.keras.Model, tf.keras.callbacks.Callback):\n    \n    def __init__(self,img_input_shape, optimizer_params, dtype):\n        super().__init__()\n#\n        self.img_input_shape = img_input_shape\n        self.optimizer_params = optimizer_params\n        self.data_type = dtype\n        self.input_layer = tf.keras.Input(self.img_input_shape, name='image', dtype= self.data_type)\n        self.random_flip = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\", seed=250)\n        \n#         self.body = swin_transformer_v2.SwinTransformerV2Tiny_window16(input_shape=self.img_input_shape,\n#                                                      num_classes=0, \n#                                                      classifier_activation = None, \n#                                                      pretrained=None)\n        #Conv2D initialization by variance scaling method\n        self.body = tf.keras.applications.efficientnet_v2.EfficientNetV2B1(\n                                include_top=None,\n                                weights=None,\n                                #input_tensor=self.input_layer,\n                                input_shape=self.img_input_shape,\n                                pooling=None,\n                                classes=0,\n                                classifier_activation=None,\n                                include_preprocessing=None)\n        \n        self.Dense = tf.keras.layers.Dense(units = 512, \n                                           activation='relu',\n                                           kernel_initializer='glorot_uniform')   \n        \n        self.pooling = tf.keras.layers.GlobalAveragePooling2D()\n        self.dropout = tf.keras.layers.Dropout(0.30)\n        self.dense_out = tf.keras.layers.Dense(units = 2, activation='softmax',kernel_initializer='glorot_uniform') \n        \n    def call(self, training, return_covmat):\n        #Add arugmentation layer\n        argu_layer = self.random_flip(self.input_layer)\n        # Add Swin transformer\n        body =  self.body(argu_layer)\n        # Average Pooling \n        conv_out = self.pooling(body)  \n        \n        output= self.dropout(conv_out)\n        output = self.dense_out(output)\n        return output\n    \n    def create_model(self):\n        \n        output = self.call(training = False,  return_covmat=False)\n        my_model = tf.keras.Model(inputs = self.input_layer, outputs = output)\n        # Optimizer    \n        optimize = tf.keras.optimizers.Adam(**self.optimizer_params)\n        # Loss\n        loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n        # Compile model\n        my_model.compile(optimizer = optimize,\n                         loss = loss_fn,\n                         metrics = ['accuracy','AUC']\n                        )\n        \n        my_model.summary()\n        return my_model","metadata":{"_uuid":"e09b2b87-c35b-432d-9018-7ed5275c568e","_cell_guid":"95c67cd8-1328-428e-8055-bc8898b65230","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-20T15:35:55.875753Z","iopub.execute_input":"2023-06-20T15:35:55.876065Z","iopub.status.idle":"2023-06-20T15:35:55.890636Z","shell.execute_reply.started":"2023-06-20T15:35:55.876036Z","shell.execute_reply":"2023-06-20T15:35:55.889746Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class PROCESS_TENSOR:\n    def __init__(self):\n        #Initilize value\n        self.zero_scalar = tf.constant(0, dtype = tf.int32, shape = ())\n        tf.print(\"Process tensor class is init!\")\n   \n    def _normalize_tensor(self,input_tensor, max_val, minval):\n        \"\"\"all dtype: tensor\"\"\"\n        return tf.math.divide(tf.math.subtract(input_tensor, minval),tf.math.subtract(max_val,minval))\n    \n    def Gen_x1_x2(self, lower_segment, upper_segment, current_img_width):\n        #Find length\n        lower_seg_length = tf.math.count_nonzero(lower_segment)\n        upper_seg_length = tf.math.count_nonzero(upper_segment)\n        \n        def fnc1():\n            #tf.print(\"True func\")\n            return  lower_segment\n        def fnc2():\n            #tf.print(\"False func\")\n            return  upper_segment\n        out_seg = tf.cond(tf.math.greater(lower_seg_length, upper_seg_length), fnc1,fnc2 )\n        \n        #Select input segment for compute\n        max_id = tf.math.argmax(out_seg,output_type = tf.int32)\n        min_id = tf.math.argmin(out_seg,output_type = tf.int32)\n        max_id = tf.reshape(max_id, shape=())\n        min_id = tf.reshape(min_id, shape=())\n        \n#         condition1 = tf.math.greater_equal( current_img_width, max_id)\n#         tf.debugging.Assert(condition1, [current_img_width, max_id, tf.shape(out_seg), image_id])\n        \n        def correct_x1():\n            #Remove end, select head\n            return self.zero_scalar , max_id\n        def correct_x2():\n            #Remove head, select end\n            return min_id , tf.math.subtract(tf.cast(current_img_width,dtype = tf.int32), min_id)\n        \n        cond_argmax_min =  tf.math.logical_and( tf.math.less( min_id , max_id ), tf.math.greater( max_id, self.zero_scalar) )\n        return tf.cond(cond_argmax_min, correct_x1, correct_x2) \n        \n    def Get_height_offset_target(self,height_dim,  input_dtype=tf.float32, output_dtype = tf.int32):\n        height_dim = tf.cast(height_dim, dtype =  input_dtype)\n        offset_height_ratio = tf.constant(0.15, dtype = input_dtype)\n        offset_height = tf.math.multiply(height_dim, offset_height_ratio)\n        target_height = tf.math.subtract(height_dim ,offset_height)\n        \n        return tf.cast(offset_height, dtype = output_dtype), tf.cast(target_height, dtype = output_dtype)\n        \n    #Crop image in tensor version\n    @tf.function\n    def Crop_Resize_tensor(self,img_tensor_input_value, bound , target_size):\n        \"\"\"\n        Operate crop img with tensor, not numpy\n        Input: \n        img_tensor_input: tf.Variable\n        curr_img_H: tensor\n        curr_img_W: tensor\n        bound: upper bound and lower bound\n        Output:\n        crped_img: tensor\n        \"\"\"\n        #Get current dimension\n        #tf.print(tf.shape(img_tensor_input_value))\n        curr_img_H = tf.slice(tf.shape(img_tensor_input_value),[0],[1])\n        curr_img_W = tf.slice(tf.shape(img_tensor_input_value),[1],[1])\n        curr_img_H = tf.reshape(curr_img_H, shape = ())\n        curr_img_W = tf.reshape(curr_img_W, shape = ())\n        #Processing image level\n        #Normailize img\n        #input: dicom image with maximum value is 32767, output: image tensor range from 0 to 1\n        img_dtype = img_tensor_input_value.dtype\n        img_tensor_input_value_norm =  self._normalize_tensor(img_tensor_input_value,\n                                                              tf.constant(32767, dtype = img_dtype),\n                                                              tf.constant(0, dtype = img_dtype)) \n        \n        # Processing histogram\n        # Calculate histogram (sum of pixel value along axis 0) of image\n        hist = tf.math.reduce_sum(img_tensor_input_value_norm, axis=0, keepdims=True)  # Have to keepdims to retain original shape\n        # Normailize hist\n        # input: histogram tensor dtype tf.float16 or the same type as input image tensor\n        norm_hist = self._normalize_tensor(hist, tf.math.reduce_max(hist),tf.constant(0, dtype = hist.dtype))\n        norm_hist = tf.squeeze(input = norm_hist)\n        # Find each segment\n        lower_seg = tf.where(tf.subtract(norm_hist,bound[0]) < 0,tf.constant(1),tf.constant(0))\n        upper_seg = tf.where(tf.subtract(norm_hist,bound[1]) > 0,tf.constant(1),tf.constant(0))\n        # Gen x1 and x2\n#         condition = tf.math.logical_and( tf.math.equal(shape1 ,curr_img_W) , tf.math.equal(shape2,curr_img_W) )\n#         tf.debugging.Assert(condition, [tf.shape(lower_seg), tf.shape(upper_seg), curr_img_W, tf.shape(hist)])\n        offset, target = self.Gen_x1_x2(lower_seg, upper_seg, curr_img_W)\n        offset_H, target_H = self.Get_height_offset_target(curr_img_H)\n        #Cropping\n        img_tensor_input_value2 =  tf.image.crop_to_bounding_box(\n                                                img_tensor_input_value_norm,\n                                                offset_height = offset_H,\n                                                offset_width = offset,\n                                                target_height = target_H,\n                                                target_width = target\n                                                             )\n \n        # Resize output\n        # Method: tensorflow.org/api_docs/python/tf/image/ResizeMethod\n        return tf.image.resize(\n                                img_tensor_input_value2,\n                                size = target_size,\n                                method=tf.image.ResizeMethod.BILINEAR,\n                                preserve_aspect_ratio=False,\n                                antialias=True\n                             )","metadata":{"_uuid":"8f9dcac2-b63f-46a9-983c-533b4c1cb0e4","_cell_guid":"a77e50db-177b-4e18-a62e-f804e815c4f0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-20T15:35:55.894277Z","iopub.execute_input":"2023-06-20T15:35:55.894569Z","iopub.status.idle":"2023-06-20T15:35:55.915513Z","shell.execute_reply.started":"2023-06-20T15:35:55.894536Z","shell.execute_reply":"2023-06-20T15:35:55.914538Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class LOADER(PROCESS_TENSOR):\n    def __init__(self, target_height, target_width, master_batch_size,writing_TRF_batch_size, bound, dtype, tfrecord_img_dir, epochs):\n        \"\"\"\n        img_dir_tensor: tensor\n        \"\"\"\n        self.target_height = target_height\n        self.target_width = target_width\n        self.master_batch_size = master_batch_size\n        self.bound = bound\n        self.main_dtype = dtype\n        self.tfrecord_img_dir = tfrecord_img_dir\n        self.writing_TRF_batch_size = writing_TRF_batch_size\n        self.epochs = epochs\n        PROCESS_TENSOR.__init__(self)\n    \n    def _decode_and_cast(self, buff_string):\n        decoded_image = tf.io.decode_jpeg(buff_string,channels=1,ratio=1, fancy_upscaling=True,try_recover_truncated=True,acceptable_fraction=1,dct_method='')\n        return tf.cast(decoded_image, dtype = self.main_dtype)\n        \n    def _sample_beta_distribution(self,concentration_0=0.2, concentration_1=0.2, seed_value = 100):\n        gamma_1_sample = tf.random.gamma(shape=[1], alpha=concentration_1, seed = seed_value)\n        gamma_2_sample = tf.random.gamma(shape=[1], alpha=concentration_0, seed = seed_value)\n        return tf.cast(tf.math.divide(gamma_1_sample ,tf.math.add(gamma_1_sample ,gamma_2_sample)) , dtype = self.main_dtype)\n    \n    def _mix_up_argument(self, img1_tensor, img2_tensor):\n        \"\"\"\n        img1 and img2 are two images that has already pre-processing and has same dimension\n        \"\"\"\n        lambda_value = self._sample_beta_distribution(concentration_0 = 0.4)\n        #lambda_value = tf.constant(1,dtype = self.main_dtype)\n        minus_lambda_value = tf.subtract(tf.constant(1, dtype = self.main_dtype),lambda_value)\n        return tf.math.add(tf.math.multiply(img1_tensor,lambda_value), tf.math.multiply(img2_tensor,minus_lambda_value))\n      \n        \n    def condition_function_argument(self, an_example):\n        return tf.math.equal(an_example[\"key\"], tf.constant([1],dtype = tf.int64))\n    \n    def true_function_argument(self, an_example):\n        \"\"\"\n        Use for normal case, no need mixup\n        \"\"\"\n        #Read and decode main image\n        #tf.print(\"True case\")\n        main_image = self._decode_and_cast(an_example[\"image\"])\n        #main_image = tf.reshape(main_image, shape = (1,main_image.shape[0],main_image.shape[1],1))\n        return self.Crop_Resize_tensor(img_tensor_input_value= main_image,\n                                                           bound = self.bound,\n                                                           target_size = [self.target_height,self.target_width]) \n    \n    def false_function_argument(self, an_example):\n        \"\"\"\n        Use for need of argumentation case\n        \"\"\"\n        #Read and decode main image\n        #tf.print(\"False case\")\n        main_image = self._decode_and_cast(an_example[\"image\"])\n        #main_image = tf.reshape(main_image, shape = (1,main_image.shape[0],main_image.shape[1],1))\n        main_image = self.Crop_Resize_tensor(img_tensor_input_value= main_image,\n                                                           bound = self.bound,\n                                                           target_size = [self.target_height,self.target_width]) \n        \n        #Read and decode paired image\n        paired_image = self._decode_and_cast(an_example[\"paired_image\"])\n        #paired_image = tf.reshape(paired_image, shape = (1,paired_image.shape[0],paired_image.shape[1],1))\n        paired_image = self.Crop_Resize_tensor(img_tensor_input_value= paired_image,\n                                                           bound = self.bound,\n                                                           target_size = [self.target_height,self.target_width]) \n        \n        return self._mix_up_argument(main_image, paired_image)\n        \n    @tf.function\n    def _read_tfrecord(self,example_row):\n        tfrecord_format = {\n                \"image\": tf.io.FixedLenFeature([], tf.string),\n                \"class\": tf.io.FixedLenFeature([], tf.int64),\n                \"paired_image\": tf.io.FixedLenFeature([], tf.string),\n                \"key\": tf.io.FixedLenFeature([], tf.int64)\n            }\n        label_format = tf.constant([[1,0],[0,1]], dtype = tf.int32)\n        # Get an example\n        example = tf.io.parse_single_example(example_row, tfrecord_format)\n        \n        #Read and decode label, class\n        label = tf.gather(params = label_format,indices = example[\"class\"])\n        \n        #Read, decode and mix images\n        temp_img = tf.cond(self.condition_function_argument(example),\n                           lambda: self.true_function_argument(example),\n                           lambda: self.false_function_argument(example))\n        tf.debugging.assert_greater_equal(temp_img, tf.constant(0, temp_img.dtype), message=None, summarize=None)\n        return (temp_img, label)\n    \n    def simple_plot(self, img_index):\n        files_path = tf.io.gfile.glob(\"/kaggle/input/dataset-ver1/training/11_900.tfrec\")\n        dataset = tf.data.TFRecordDataset(files_path,\n                                          compression_type= 'GZIP',\n                                          buffer_size = 300,\n                                          num_parallel_reads = 4)  \n        dataset = dataset.map(lambda x : self._read_tfrecord(x),\n                              num_parallel_calls=tf.data.AUTOTUNE)\n        dataset = dataset.take(3)\n        dataset = dataset.enumerate(start = 0)\n        #tf.print(\"Lenght of dataset: \",)\n        for order, (image,label) in dataset.as_numpy_iterator():\n            if order == img_index :\n                tf.print(\"image shape: \",image.shape)\n                tf.print(\"image type: \",image.dtype)\n                tf.print(\"label: \", label)\n                \n                image = tf.math.multiply(image,tf.constant(32767,dtype = image.dtype))\n                #rescle data for plotting\n                image = tf.cast(image, dtype = tf.float16)\n    \n                plt.imshow(image, cmap = \"gray\")\n                plt.show\n    \n    def create_dataset(self,strategy, num_replicas):\n        options = tf.data.Options()\n        options.threading.private_threadpool_size = 3\n        options.experimental_deterministic = False  # disable order, increase speed\n        files_path = tf.io.gfile.glob(self.tfrecord_img_dir + \"*.tfrec\")\n        tf.print(\"File list: \", files_path)\n        dataset = tf.data.TFRecordDataset(files_path,\n                                          compression_type= 'GZIP',\n                                          buffer_size = 120,\n                                          num_parallel_reads = 4)  # automatically interleaves reads from multiple files\n        \n        dataset = dataset.with_options(options)  # uses data as soon as it streams in, rather than in its original order\n        dataset = dataset.map(lambda x : self._read_tfrecord(x),\n                              num_parallel_calls=tf.data.AUTOTUNE)\n        dataset = dataset.batch(self.master_batch_size,drop_remainder = True)\n        \n        dataset = dataset.prefetch(buffer_size = tf.data.AUTOTUNE)\n        dataset = dataset.repeat(self.epochs)\n        #Find actual length of input dataset\n        dataset_length = int(len(files_path)*self.writing_TRF_batch_size//(self.master_batch_size*num_replicas))\n            #Distributed the dataset over devices\n        #dist_dataset = strategy.experimental_distribute_dataset(dataset)\n        #dataset = dataset.shuffle(buffer_size = dataset_length)\n        \n        return dist_dataset , dataset_length","metadata":{"_uuid":"1277d2ec-bd05-4753-b9b2-fc9e00de0290","_cell_guid":"f800fa74-6c49-4791-af59-9e6cdd060944","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-20T15:35:55.916940Z","iopub.execute_input":"2023-06-20T15:35:55.917278Z","iopub.status.idle":"2023-06-20T15:35:55.947537Z","shell.execute_reply.started":"2023-06-20T15:35:55.917248Z","shell.execute_reply":"2023-06-20T15:35:55.946628Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def scheduler(epoch, lr):\n    if epoch > 2:\n        return lr\n    else:\n        return lr * tf.math.exp(-0.2)\n\ndef Training_model(model_version, model_inst, train_gen_obj, vali_gen_obj, EPOCHS,train_steps, vali_steps, VERBOSE, num_of_workers):\n    \n    model = model_inst.create_model()\n    \n    #Learning rate callback\n    LR_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n    \n    # Define the ProgbarLogger callback\n    progbar_callback = tf.keras.callbacks.ProgbarLogger(count_mode= 'samples',stateful_metrics='accuracy')\n    # Checkpoint callback\n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n                                        filepath='/kaggle/working/best_weight_'+model_version+'.h5',\n                                        save_weights_only=True,\n                                        monitor='val_accuracy',\n                                        mode='max',\n                                        verbose = 1,\n                                        save_best_only=True)\n \n    # Fit model with data\n    history = model.fit(\n            x=train_gen_obj,\n            epochs=EPOCHS,\n            verbose=1,\n            callbacks=[model_checkpoint_callback, progbar_callback, LR_callback],\n            validation_data=vali_gen_obj,\n            workers = num_of_workers,\n            use_multiprocessing = False,\n            steps_per_epoch= train_steps,\n            validation_steps = vali_steps\n            )\n    return history","metadata":{"_uuid":"d0e62589-d8ce-45a1-98fb-f8eabfc4694d","_cell_guid":"bfe5a5cf-49be-4d1e-87be-b0f81ece25a6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-20T15:35:55.948980Z","iopub.execute_input":"2023-06-20T15:35:55.949321Z","iopub.status.idle":"2023-06-20T15:35:55.958867Z","shell.execute_reply.started":"2023-06-20T15:35:55.949292Z","shell.execute_reply":"2023-06-20T15:35:55.957927Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Parameter\nk = -2\nimg_height = 1024 - k*128\nimg_width = 512 - k*64\nimg_shape = (img_height, img_width,1)\nepochs = 300\nverbose = 1\n\n#gcs_path = KaggleDatasets().get_gcs_path(\"rsna-breast-cancer-detection\")\n#train_default_path = f'{gcs_path}/train_images/'\ntrain_default_path = f'/kaggle/input/rsna-breast-cancer-detection/train_images/'\ntest_default_path = f'/kaggle/input/rsna-breast-cancer-detection/test_images/'\n# # Test file paths\n#DICOM_FILE_PATHS = sorted(tf.io.gfile.glob(f'{gcs_path}/train_images/*/*.dcm'))\n#print(f'Found {len(DICOM_FILE_PATHS)} DICOM files')\n\ntfrecord_train_path = f'/kaggle/input/dataset-ver1/training/'\ntfrecord_vali_path = f'/kaggle/input/dataset-ver1/validation/'\n#num_replicas_in_sync = strategy.num_replicas_in_sync\n#Batch file for writing file faster\ntrain_TFR_batch_size = 600\nvali_TFR_batch_size = 200\n\n#Batch for training process\ntrain_vali_ratio = 0.4\nnum_replicas_in_sync = strategy.num_replicas_in_sync\nvali_batch_size = 8*num_replicas_in_sync   \ntrain_batch_size = int(vali_batch_size//train_vali_ratio)\n\nver_model = \"ver_2\"\n\nseed = 150\nbound = [0.2, 0.8]\noptimizer_params = dict(learning_rate = 0.00001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-7, weight_decay = 0.00001*0.01)\nnum_of_workers = 2\nmaster_img_dtype = tf.float32\n#Create random object for shullfe\nrand_obj = np.random.RandomState(seed = seed)\n\n#Check directory\nif os.path.exists('/kaggle/working/training') == False:\n    os.makedirs('/kaggle/working/training')\nif os.path.exists('/kaggle/working/validation') == False:\n    os.makedirs('/kaggle/working/validation')","metadata":{"_uuid":"17c6f8f5-0043-44fd-b067-5f25b21405d4","_cell_guid":"806c9ea3-dd62-44f3-bd4e-f1b9242dac30","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-20T15:35:55.960312Z","iopub.execute_input":"2023-06-20T15:35:55.960664Z","iopub.status.idle":"2023-06-20T15:35:55.974718Z","shell.execute_reply.started":"2023-06-20T15:35:55.960630Z","shell.execute_reply":"2023-06-20T15:35:55.973869Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"with strategy.scope(): \n    #Runing session\n    main_instance = Make_TFRecord(20000,img_height, img_width,train_vali_ratio, master_img_dtype, epochs, rand_obj)\n    main_instance.write_to_tfrecord(main_instance.vali_dataset,\n                                    vali_TFR_batch_size,\n                                    '/kaggle/working/validation',\n                                    50,\n                                    strategy)\n    \n    main_instance.write_to_tfrecord(main_instance.train_dataset,\n                                    train_TFR_batch_size,\n                                    '/kaggle/working/training', \n                                    100,\n                                    strategy)\n    tf.print(\"Finish writing TFRecords\")\n    \n    main_instance = None\n    del main_instance","metadata":{"_uuid":"feff5b2a-42bc-4e36-b975-6279ea0a9575","_cell_guid":"0a13577f-f201-4d06-90de-acebcd91bc1a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-20T15:35:55.976141Z","iopub.execute_input":"2023-06-20T15:35:55.976479Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Before applying MixUP method: positive class :  251 images,  negative class:  11748 images\nBefore applying MixUP method: total size of training set :  11999 images\nBefor applying MixUP method, in positive class: \n   Laterality L view:  125 images\n   Laterality R view:  126 images\n","output_type":"stream"}]},{"cell_type":"code","source":"#Master training model\nwith strategy.scope():\n    model_instance = Main_Architecture(img_input_shape = img_shape, optimizer_params = optimizer_params, dtype = master_img_dtype)\n    \n    # Create data loading object   target_height, target_width, master_batch_size, bound, dtype, tfrecord_img_dir\n    Train_loader = LOADER(img_height,img_width, train_batch_size, train_TFR_batch_size, bound, master_img_dtype, tfrecord_train_path , epochs)\n    Vali_loader = LOADER(img_height,img_width, vali_batch_size, vali_TFR_batch_size, bound, master_img_dtype, tfrecord_vali_path, epochs)\n    # Return a dataset\n    train_dataset, train_steps = Train_loader.create_dataset(strategy, num_replicas_in_sync)\n    vali_dataset, vali_steps = Vali_loader.create_dataset(strategy, num_replicas_in_sync)\n    \n    tf.print(\"Ready for training\")\n    #Training model\n    history_train = Training_model(ver_model,model_instance, train_dataset, vali_dataset, \n                                       epochs,\n                                       train_steps,\n                                       vali_steps,\n                                       verbose, num_of_workers)\n    #summarize history for accuracy\n    plt.plot(history_train.history['accuracy'])\n    plt.plot(history_train.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    \n    plt.show()\n    # summarize history for loss\n    plt.plot(history_train.history['loss'])\n    plt.plot(history_train.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    \n    train_dataset = None\n    vali_dataset = None\n    \n    del Train_loader\n    del Vali_loader","metadata":{"_uuid":"31244982-c60a-4f5d-9bf4-7d4119e7588b","_cell_guid":"be86f6ad-5c9c-4eb8-ad72-9cf23758e3a3","collapsed":false,"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"9167c2c3-cb7b-44af-a2c8-210e58c3c647","_cell_guid":"e60644c7-c251-4245-ba9e-e247932eba4f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _extract_infor():\n    test_list = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/test.csv')\n    test_frame = pd.DataFrame(data={'patient_id': test_list.patient_id\n                                    ,'image_id': test_list.image_id}\n                              ,columns = ['patient_id','image_id'])\n    return tf.convert_to_tensor(test_frame.values, dtype = tf.int32)  \n \ndef _dicom_to_image_test(infor_tensor,target_img_height,target_img_width):\n    patient_id = tf.slice(infor_tensor,[0],[1])\n    img_id = tf.slice(infor_tensor,[1],[1])\n    patient_id = tf.reshape(patient_id, shape = ())\n    img_id = tf.reshape(img_id, shape = ())\n    train_default_path = f'/kaggle/input/rsna-breast-cancer-detection/test_images/'\n    string_tensor_path = tf.strings.format(train_default_path+'{}/{}.dcm', (patient_id,img_id))\n    string_np_path = string_tensor_path.numpy().decode()\n    dcm_file = dcmread(string_np_path, force = True)\n    image_array = dcm_file.pixel_array\n    image_array = image_array.reshape((image_array.shape[0],image_array.shape[1],1))\n    return tf.cast(tf.image.resize(image_array,size = (target_img_height,target_img_width)), dtype = tf.float16)   \n\ndef _dicom_to_image(infor_tensor,target_img_height,target_img_width):\n    return tf.py_function(_dicom_to_image_test,inp=[infor_tensor,target_img_height,target_img_width],Tout = tf.float16)\n    \n    \ndef create_dataset_and_processing(target_img_height,target_img_width, bound):\n    test_dataset = tf.data.Dataset.from_tensor_slices(_extract_infor())\n    test_dataset = test_dataset.map(lambda x: _dicom_to_image(x,target_img_height,target_img_width))\n    process_tensor_instance = PROCESS_TENSOR()\n    test_dataset = test_dataset.map(lambda y: process_tensor_instance.Crop_Resize_tensor(y,bound , [target_img_height,target_img_width]))\n    \n    return test_dataset\n\ndef Inference():\n    #Init new instance\n    model_instance = Main_Architecture(img_input_shape = img_shape, optimizer_params = optimizer_params, dtype = master_img_dtype)\n    inference_model = model_instance.create_model()\n    #Load trained weights\n    inference_model.load_weights('/kaggle/working/best_weight_ver_2.h5')\n    \n    #Make predict on processed images in form of tf.data.Dataset\n    return inference_model.predict(create_dataset_and_processing())","metadata":{"_uuid":"ec28d5bf-268c-4df2-8636-8bf5b722d4c1","_cell_guid":"bb3d9c78-39c5-46f3-8e6a-c593567a390f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\n# sample_submission = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/sample_submission.csv')\n\n# display(sample_submission.info())\n# display(sample_submission.head())\n\nscores = Inference()\nscores = tf.slice(scores, [0,1],[scores.shape[0],1])\nscores = pd.DataFrame(scores, columns = ['cancer']).astype(\"float\")\ntf.print(scores)\ntest_list = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/test.csv')\ntest_result = pd.DataFrame(data={'patient_id': test_list.patient_id\n                                    ,'image_id': test_list.image_id\n                                    ,'prediction_id': test_list.prediction_id\n                                    , 'cancer': scores.cancer}\n                              ,columns = ['patient_id','image_id','prediction_id','cancer'])\n# Save submission as CSV\ntest_result.to_csv('submission.csv', index=False)\n# show result\npd.read_csv('/kaggle/working/submission.csv')","metadata":{"_uuid":"14f22609-a37e-456f-a3f0-015aac077dcc","_cell_guid":"b03b4fa4-ada9-46a5-b172-31e1f43989e1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}